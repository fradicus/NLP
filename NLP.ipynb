{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyA+Ex/5eDa+LARUJPA7YN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fradicus/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Natural Language Processing**\n",
        "\n",
        "Import handling\n",
        "```\n",
        "nltk - Natural Language Toolkit\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "eixp5jj8vkFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKAIHutiveXD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Dataset input**\n",
        "\n",
        "Read in semi-strucutred data"
      ],
      "metadata": {
        "id": "BHY9d1-9vjJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take in raw text (CHANGE WORDING)\n",
        "userPrompt = open(\"inputpoop.tsv\").read()\n",
        "# Print data\n",
        "userPrompt[0:250]"
      ],
      "metadata": {
        "id": "7eescgRTx4Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Interpreting Data**\n",
        "\n",
        "Using Pandas to help translate our semi-structured data, into a format that can be understood by a machine\n"
      ],
      "metadata": {
        "id": "S89zHU7E0QC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Taking in tab separated value\n",
        "inputData = pd.read_csv('inputpoop.tsv', sep='\\t', names=['label', 'body_text'], header=None)\n",
        "# Print first 5 data\n",
        "inputData.head()"
      ],
      "metadata": {
        "id": "Zk5KhtUe1D4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Cleaning our Data**\n",
        "\n",
        "4a.) Removing Punctuation\n",
        "```\n",
        "Our vectorizer is only interested in the number of words within the user's prompt.\n",
        "Punctuation is irelevant, special characters are removed. (CHANGE \"removed\")\n",
        "```"
      ],
      "metadata": {
        "id": "19vW-Dtq2xXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "4P5EzMW_319t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delPunc(text)\n",
        "  dePuntuate = \"\".join([char for char in text if char not in string.punctuation])\n",
        "  return dePuntuate\n",
        "inputData['cleanBody'] = inputData['body_text'].apply(lambda x: delPunc(x))\n",
        "\n",
        "inputData.head()"
      ],
      "metadata": {
        "id": "5tLTGC-K4bSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.b) Tokenize our Text      \n",
        "```\n",
        "Separate the text from user prompt into indivdual words or \"tokens\",\n",
        "providing structure to formerly unstructured text.\n",
        "```"
      ],
      "metadata": {
        "id": "uBuDFNXO6hIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    token = re.split('\\W+', text) #W+ means that either a word character (A-Za-z0-9_) or a dash (-) can go there. (huh? change this)\n",
        "    return token\n",
        "\n",
        "# Lowercaseing our text (change wording)\n",
        "inputData['tokenText'] = inputData['cleanBody'].apply(lambda x: tokenize(x.lower()))\n",
        "\n",
        "inputData.head()"
      ],
      "metadata": {
        "id": "BJxlE3dn7a5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.c) Removing Stopwords     \n",
        "```\n",
        "We delete unnecessary verbage or \"stopwords\" that are solely useful for human communication,\n",
        "to focus only on what the text tells us about our data.\n",
        "This process is handled in 29 of the most common spoken languages.\n",
        "```"
      ],
      "metadata": {
        "id": "f9SZpTgfAHll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = (nltk.corpus.stopwords.words('arabic') +\n",
        "             nltk.corpus.stopwords.words('azerbaijani') +\n",
        "             nltk.corpus.stopwords.words('basque') +\n",
        "             nltk.corpus.stopwords.words('bengali') +\n",
        "             nltk.corpus.stopwords.words('catalan') +\n",
        "             nltk.corpus.stopwords.words('chinese') +\n",
        "             nltk.corpus.stopwords.words('danish') +\n",
        "             nltk.corpus.stopwords.words('dutch') +\n",
        "             nltk.corpus.stopwords.words('english') +\n",
        "             nltk.corpus.stopwords.words('finnish') +\n",
        "             nltk.corpus.stopwords.words('french') +\n",
        "             nltk.corpus.stopwords.words('german') +\n",
        "             nltk.corpus.stopwords.words('greek') +\n",
        "             nltk.corpus.stopwords.words('hebrew') +\n",
        "             nltk.corpus.stopwords.words('hinglish') +\n",
        "             nltk.corpus.stopwords.words('hungarian') +\n",
        "             nltk.corpus.stopwords.words('indonesian') +\n",
        "             nltk.corpus.stopwords.words('italian') +\n",
        "             nltk.corpus.stopwords.words('kazakh') +\n",
        "             nltk.corpus.stopwords.words('nepali') +\n",
        "             nltk.corpus.stopwords.words('norwegian') +\n",
        "             nltk.corpus.stopwords.words('portuguese') +\n",
        "             nltk.corpus.stopwords.words('romanian') +\n",
        "             nltk.corpus.stopwords.words('russian') +\n",
        "             nltk.corpus.stopwords.words('slovene') +\n",
        "             nltk.corpus.stopwords.words('spanish') +\n",
        "             nltk.corpus.stopwords.words('swedish') +\n",
        "             nltk.corpus.stopwords.words('tajik') +\n",
        "             nltk.corpus.stopwords.words('turkish'))\n"
      ],
      "metadata": {
        "id": "cvN8WubqBDb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delStopwords(tokenList):\n",
        "  text = [word for word in tokenList if word not in stopwords]# To remove all stopwords\n",
        "  return text\n",
        "\n",
        "inputData['body_text_nostop'] = inputData['tokenText'].apply(lambda x: delStopwords(x))\n",
        "\n",
        "inputData.head()"
      ],
      "metadata": {
        "id": "kpqLuQXqISnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.d) Stemming our data     \n",
        "```\n",
        "As the term implies, we are breaking down a word to its main component.\n",
        "We remove suffixes, transforming our word into its \"stem form\".\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "K_AdJ6yTLbfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "def stemming(tokendText):\n",
        "    text = [ps.stem(word) for word in tokendText]\n",
        "    return text\n",
        "\n",
        "inputData['stemmedText'] = inputData['body_text_nostop'].apply(lambda x: stemming(x))\n",
        "\n",
        "inputData.head()"
      ],
      "metadata": {
        "id": "b8Aw-XbsMT18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4e.) Lemmatizing\n",
        "\n",
        "```\n",
        "We must find the base of a word through a dictionary-based approach, performing a morphological analysis.\n",
        "This permits the restructure of our words into their constituent pieces, while understanding their meanings.\n",
        "```"
      ],
      "metadata": {
        "id": "39YaGCq_OzPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordNet = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatizing(tokenText):\n",
        "    text = [wordNet.lemmatize(word) for word in tokenText]\n",
        "    return text\n",
        "\n",
        "inputData['lemmatizedText'] = inputData['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
        "\n",
        "inputData.head(10)"
      ],
      "metadata": {
        "id": "2lDEAWc_RFcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4f.) Saving our Data\n",
        "\n",
        "\n",
        "```\n",
        "It is wise to save our file now as a .csv, rather than the former .tsv,\n",
        "allowing our file to be easier to read.\n",
        "```"
      ],
      "metadata": {
        "id": "_Uk7EhAYTA-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Vectorizing our Data**\n",
        "```\n",
        "Converting our text to integers, in order to create feature vectors,\n",
        "which permit our model to understand lanugage.\n",
        "```"
      ],
      "metadata": {
        "id": "E743MbSbVKLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5a.) Bag-Of-Words (BoW) Model\n",
        "\n",
        "```\n",
        "We create a \"bag of words\", by determining whether or not a word is found within our data.\n",
        "```"
      ],
      "metadata": {
        "id": "Dsm3CTzbWnog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "pd.set_option('display.max_colwidth', 100) # (change variable name)\n",
        "\n",
        "stopwords = (nltk.corpus.stopwords.words('arabic') +\n",
        "             nltk.corpus.stopwords.words('azerbaijani') +\n",
        "             nltk.corpus.stopwords.words('basque') +\n",
        "             nltk.corpus.stopwords.words('bengali') +\n",
        "             nltk.corpus.stopwords.words('catalan') +\n",
        "             nltk.corpus.stopwords.words('chinese') +\n",
        "             nltk.corpus.stopwords.words('danish') +\n",
        "             nltk.corpus.stopwords.words('dutch') +\n",
        "             nltk.corpus.stopwords.words('english') +\n",
        "             nltk.corpus.stopwords.words('finnish') +\n",
        "             nltk.corpus.stopwords.words('french') +\n",
        "             nltk.corpus.stopwords.words('german') +\n",
        "             nltk.corpus.stopwords.words('greek') +\n",
        "             nltk.corpus.stopwords.words('hebrew') +\n",
        "             nltk.corpus.stopwords.words('hinglish') +\n",
        "             nltk.corpus.stopwords.words('hungarian') +\n",
        "             nltk.corpus.stopwords.words('indonesian') +\n",
        "             nltk.corpus.stopwords.words('italian') +\n",
        "             nltk.corpus.stopwords.words('kazakh') +\n",
        "             nltk.corpus.stopwords.words('nepali') +\n",
        "             nltk.corpus.stopwords.words('norwegian') +\n",
        "             nltk.corpus.stopwords.words('portuguese') +\n",
        "             nltk.corpus.stopwords.words('romanian') +\n",
        "             nltk.corpus.stopwords.words('russian') +\n",
        "             nltk.corpus.stopwords.words('slovene') +\n",
        "             nltk.corpus.stopwords.words('spanish') +\n",
        "             nltk.corpus.stopwords.words('swedish') +\n",
        "             nltk.corpus.stopwords.words('tajik') +\n",
        "             nltk.corpus.stopwords.words('turkish'))\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "inputData = pd.read_csv(\"inputpoop.tsv\", sep='\\t') # is this supposed to inputpoop.csv (pd.read_csv)\n",
        "inputData.columns = ['label', 'body_text']"
      ],
      "metadata": {
        "id": "_s0z-V4tUFV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning our text\n",
        "\n",
        "```\n",
        "This step eliminates our punctuation, tokenizes, deletes stopwords, and stem.\n",
        "```"
      ],
      "metadata": {
        "id": "BoIm2xipZdo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaningText(text):\n",
        "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "    token = re.split('\\W+', text)\n",
        "    text = [ps.stem(word) for word in token if word not in stopwords]\n",
        "    return text"
      ],
      "metadata": {
        "id": "vRKFv2G4ataT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5a.) (Continuted) - CountVectorizer\n",
        "\n",
        "```\n",
        "Using a specific implementation of the BoW model, known as CountVectorizer, we store the count of each word in our document matrix.\n",
        "We transform the text data into a matrix of token counts, where each row represents a document and each column represents a word from the vocabulary.\n",
        "```\n"
      ],
      "metadata": {
        "id": "WmXfPdnJbaAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "countVec = CountVectorizer(analyzer=cleaningText)\n",
        "X_counts = countVec.fit_transform(inputData['body_text'])\n",
        "print(X_counts.shape)\n",
        "print(countVec.get_feature_names())"
      ],
      "metadata": {
        "id": "GQuTbbLDbsT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5b.) N-Grams\n",
        "\n",
        "```\n",
        "Here we use a concept that gives us the opportunity to capture the letter or word, that is expected to follow a given word.\n",
        "We're searching for all combinations of trailing or adjacent letters/words\n",
        "of variable length 'n'.\n",
        "```\n"
      ],
      "metadata": {
        "id": "FiIaxq5BerNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "ngramVect = CountVectorizer(ngram_range=(2,2),analyzer=cleaningText)\n",
        "X_counts = ngramVect.fit_transform(inputData['body_text'])\n",
        "print(X_counts.shape)\n",
        "print(ngramVect.get_feature_names())"
      ],
      "metadata": {
        "id": "ELsL_55kjMu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5c.) TF-IDF\n",
        "```\n",
        "Term Frequency (TF): Measures how frequently a term appears in a document.\n",
        "Inverse Document Frequency (IDF): Measures how important a term is.\n",
        "We're finding the relative frequency of a word, as it appears in a document, compared to all docs.\n",
        "```\n",
        "Equation: wᵢⱼ = tfᵢ,ⱼ × log(N / dfᵢ)"
      ],
      "metadata": {
        "id": "DAwnVkwVnPKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidfVect = TfidfVectorizer(analyzer=cleaningText)\n",
        "X_tfidf = tfidfVect.fit_transform(inputData['body_text'])\n",
        "print(X_tfidf.shape)\n",
        "print(tfidfVect.get_feature_names())"
      ],
      "metadata": {
        "id": "qdiQNJO6obe6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}